<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images</title>
	<meta property="og:image" content="Path to my teaser.png"/> <!-- Facebook automatically scrapes this. Go to https://developers.facebook.com/tools/debug/ if you update and want to force Facebook to rescrape. -->
	<meta property="og:title" content="RAW-Adapter: Adapting Pre-trained Visual Model to Camera RAW Images" />
	<meta property="og:description" content="Paper description." />

	<!-- Get from Google Analytics -->
	<!-- Global site tag (gtag.js) - Google Analytics -->
	<script async src=""></script> 
	<script>
		window.dataLayer = window.dataLayer || [];
		function gtag(){dataLayer.push(arguments);}
		gtag('js', new Date());

		gtag('config', 'UA-75863369-6');
	</script>
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">[ECCV 2024] RAW-Adapter: Adapting Pre-trained <br> Visual Model to Camera RAW Images</span>
		<table align=center width=600px>
			<table align=center width=600px>
				<tr>
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://cuiziteng.github.io/">Ziteng Cui<sup>1</sup></a></span>
						</center>
					</td>
					
					<td align=center width=180px>
						<center>
							<span style="font-size:18px"><a href="https://www.mi.t.u-tokyo.ac.jp/harada/">Tatsuya Harada<sup>1,2</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
			
			<table align=center width=400px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:14px"><a>1. The University of Tokyo</a></span>
						</center>
					</td>
					
					<td align=center width=120px>
						<center>
							<span style="font-size:14px"><a>2. RIKEN AIP</a></span><br>
						</center>
					</td>
				</tr>
			</table>
			
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href=''>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:18px"><a href='https://github.com/cuiziteng/ECCV_RAW_Adapter'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>

	<center>
		<table align=center width=1000px>
			<tr>
				<td width=850px>
					<center>
						<img class="round" style="width:800px" src="./resources/Fig1.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=1000px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
			sRGB images are now the predominant choice for pre-training visual models in computer vision research, owing to their ease of acquisition and efficient storage. Meanwhile, the advantage of RAW images lies in their rich physical information under variable real-world challenging lighting conditions. For computer vision tasks directly based on camera RAW data, most existing studies adopt 
			methods of integrating image signal processor (ISP) with backend networks, yet often overlook the interaction capabilities between the ISP stages and subsequent networks. 
			Drawing inspiration from ongoing adapter research in NLP and CV areas, we introduce <b>RAW-Adapter</b>, a novel approach aimed at adapting sRGB pre-trained models to camera RAW data. RAW-Adapter comprises input-level adapters that employ learnable ISP stages to adjust RAW inputs, as well as model-level adapters to build connections between ISP stages and subsequent high-level networks.
			Additionally, RAW-Adapter is a general framework that could be used in various computer vision frameworks. Abundant experiments under different lighting conditions have shown our algorithm's state-of-the-art (SOTA) performance, demonstrating its effectiveness and efficiency across a range of real-world and synthetic datasets.
			</td>
		</tr>
	</table>

	<br>

	
	<center>
		<table align=center width=1000px>
			<tr>
				<td width=850px>
					<center>
						<img class="round" style="width:800px" src="./resources/Supp_Fig1.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<br>
	
	<center>
		<table align=center width=1000px>
			<tr>
				<td width=850px>
					<center>
						Performance of RAW-based visual tasks with and without sRGB pre-trained weights. Two methods: Dirty-Pixel (Siggraph 2021) and RAW-Adapter. 
						Blue line means trained with MS COCO pre-train weights, purple line indicates ImageNet pre-train weights, the yellow line indicates training from scratch.
						Using <b>sRGB pretrain weights</b> is crucial in <b>RAW-based vision tasks</b>.
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	
	<br>



	
	<center><h1>Methods</h1></center>

	<center>
		<table width=1000px>
			<tr>
				<td width=850px>
					
					RAW-Adapter is a general framework that could be built on various network backbone, it mainly includes 2 parts: <b>Input-level Adapter</b> and <b>Model-level Adapters</b> <br/>
					<br>
					<b>Input-level Adapter :</b> Including (1). dynamic enhancement, (2). denoise and (3). sharpen (Predicted by attention block P_K), as well as (4). white balance and (5).CCM (Predicted by attention block P_M) and (6) implicit 3D LUT <br/>
					<br>
					<b>Model-level Adapter :</b> Building the connection of above ISP part representation and model level features, dotted line in following pictures.
				</td>
			</tr>
		</table>
	</center>
	<br>
	<center>
		<table align=center width=1000px>
			<tr>
				<td width=850px>
					<center>
						<img class="round" style="width:800px" src="./resources/Fig2.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>
	
	

	<center>
		<table align=center width=1000px>
			<tr>
				<td width=850px>
					<center>
						<img class="round" style="width:800px" src="./resources/Fig3.png"/>
					</center>
				</td>
			</tr>
		</table>
	</center>

	<hr>
	
	<center><h1>Experiments</h1></center>

	<table align=center width=420px>
		<center>
			<tr>
				<td>
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=400px>
		<tr>
			<td align=center width=400px>
				<center>
					<td><img class="round" style="width:450px" src="./resources/method_diagram.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Short description if wanted
				</td>
			</tr>
		</center>
	</table>
	<table align=center width=800px>
		<br>
		<tr><center>
			<span style="font-size:28px">&nbsp;<a href='https://github.com/richzhang/webpage-template'>[GitHub]</a>
			</center>
		</span>
	</table>
	<br>

	
	<hr>
	<table align=center width=800px>
		<center><h1>Our Related Research</h1></center>
		
		<tr>
			Aleth-NeRF: Illumination Adaptive NeRF with Concealing Field Assumption. <b>AAAI 2024.</b> <a href='https://cuiziteng.github.io/Aleth_NeRF_web/'>[Link] <a href='https://arxiv.org/pdf/2312.09093'>[Paper]</a>
			 <br>
			You Only Need 90K Parameters to Adapt Light: a Light Weight Transformer for Image Enhancement and Exposure Correction.
				<b>BMVC 2022.</b> <a href='https://github.com/cuiziteng/Illumination-Adaptive-Transformer'>[Link] <a href='https://bmvc2022.mpi-inf.mpg.de/238/'>[Paper]</a> <br>
			Multitask AET with Orthogonal Tangent Regularity for Dark Object Detection.
				<b>ICCV 2021.</b> <a href='https://github.com/cuiziteng/ICCV_MAET'>[Link] <a href='https://openaccess.thecvf.com/content/ICCV2021/papers/Cui_Multitask_AET_With_Orthogonal_Tangent_Regularity_for_Dark_Object_Detection_ICCV_2021_paper.pdf'>[Paper]</a> <br>
		</tr>
	</table>
	<br>
	<hr>
	<table align=center width=800px>
		    <center><h1>BibTeX</h1></center>
		    <pre><code>
		 @inproceedings{raw_adapter,
		  title     = {RAW-Adapter: Adapting Pretrained Visual Model to Camera RAW Images},
		  author    = {Cui, Ziteng and Harada, Tatsuya},
		  booktitle={ECCV},
  		  year={2024}
		}
		    </code></pre>
	
	</table>
					
	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

</body>
</html>

